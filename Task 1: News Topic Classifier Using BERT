# Task 1: News Topic Classifier Using BERT
## 1. Problem Statement & Objective
### Problem Statement:
News articles come in various categories like World, Sports, Business, and Science/Technology. Manually categorizing them is time-consuming.

### Objective:
Build a machine learning model using a pre-trained BERT transformer to automatically classify news headlines into one of four categories.

You will:

*   Load the AG News dataset
*   Tokenize and fine-tune a BERT model
*   Evaluate performance using accuracy and F1 score
*   Optionally deploy the model using Streamlit or Gradio


## Libraries Install

!pip install --upgrade --force-reinstall \
  transformers==4.38.2 \
  accelerate==0.27.2 \
  peft==0.10.0 \
  torch==2.6.0 \
  torchvision==0.21.0 \
  torchaudio==2.6.0 \
  pandas==2.2.2 \
  numpy==1.26.4 \
  fsspec==2025.3.2 \
  pyarrow==14.0.0 \
  scikit-learn

!pip install --upgrade --force-reinstall numpy
!pip install numpy==1.24.4 --force-reinstall

import importlib.util

def check_package(package_name):
    spec = importlib.util.find_spec(package_name)
    if spec is not None:
        try:
            module = __import__(package_name)
            print(f" {package_name} is installed (version: {module.__version__})")
        except AttributeError:
            print(f" {package_name} is installed")
    else:
        print(f" {package_name} is NOT installed")

#  Check essential packages for BERT fine-tuning
required_packages = [
    "transformers",
    "torch",
    "torchvision",
    "torchaudio",
    "peft",
    "accelerate",
    "pandas",
    "numpy",
    "fsspec",
    "pyarrow",
    "sklearn"
]

for pkg in required_packages:
    check_package(pkg)

## 2. Dataset Loading & Preprocessing

!mkdir -p ag_news
!wget -O ag_news/train.csv https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv
!wget -O ag_news/test.csv https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv


## Only 2000 samples were taken because training the full dataset would take around 60 hours.

import pandas as pd

# Load the CSV files
train_df = pd.read_csv("ag_news/train.csv", header=None)
test_df = pd.read_csv("ag_news/test.csv", header=None)

# Rename columns for clarity
train_df.columns = ["label", "title", "description"]
test_df.columns = ["label", "title", "description"]

# Convert labels from 1â€“4 to 0â€“3
train_df["label"] = train_df["label"] - 1
test_df["label"] = test_df["label"] - 1


# Reduce to 2000 samples for quick testing
train_df = train_df.sample(n=2000, random_state=42).reset_index(drop=True)
test_df = test_df.sample(n=500, random_state=42).reset_index(drop=True)


train_df.head()

## 3. Tokenize News Titles with BERT Tokenizer

from transformers import AutoTokenizer

# Load BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")


# Sample data
titles = train_df["title"].tolist()
labels = train_df["label"].tolist()

tokenized = tokenizer(
    titles,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

import torch
from torch.utils.data import Dataset

class NewsDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Create dataset
train_dataset = NewsDataset(tokenized, labels)


test_titles = test_df["title"].tolist()
test_labels = test_df["label"].tolist()

test_tokenized = tokenizer(
    test_titles,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

test_dataset = NewsDataset(test_tokenized, test_labels)


## 4. Model Development & Training (Fine-Tuning BERT)
Load Pre-trained BERT Model

from transformers import AutoModelForSequenceClassification

# Load model: 4 output classes (World, Sports, Business, Sci/Tech)
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=4)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_steps=50,
    logging_dir='./logs',
    save_steps=500,
    report_to=[],  # disables wandb
)

from sklearn.metrics import accuracy_score, f1_score
import numpy as np

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="macro")
    return {"accuracy": acc, "f1": f1}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)


trainer.train()

## 5. Evaluate the Model

# Evaluate on test data
eval_result = trainer.evaluate(eval_dataset=test_dataset)
print(" Evaluation results:")
print(eval_result)


## 6. Make Predictions

# Map label numbers to actual category names
label_map = {
    0: "World",
    1: "Sports",
    2: "Business",
    3: "Sci/Tech"
}

# Pick some test samples
sample_titles = test_df["title"].iloc[:5].tolist()

# Tokenize
inputs = tokenizer(
    sample_titles,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

# Run model in eval mode
model.eval()
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=1)

# Show results
for i, title in enumerate(sample_titles):
    pred_label = predictions[i].item()
    print(f" Title: {title}")
    print(f" Predicted label: {label_map[pred_label]}")
    print("---")

custom_titles = [
    "Apple releases new iPhone with AI camera features",
    "NASA plans mission to explore Europa",
    "Cristiano Ronaldo scores winning goal for Portugal",
    "Stock market drops after inflation report"
]

# Tokenize your input
inputs = tokenizer(
    custom_titles,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

# Get predictions
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, axis=1)

# Label mapping
label_map = {0: "World", 1: "Sports", 2: "Business", 3: "Sci/Tech"}

# Display results
for i, title in enumerate(custom_titles):
    print(f" Title: {title}")
    print(f" Predicted label: {label_map[predictions[i].item()]}")
    print("---")


## 7. Save the Model

model.save_pretrained("bert-news-topic-model")
tokenizer.save_pretrained("bert-news-topic-model")

## 8. Gardio setup

!pip install gradio

import gradio as gr
import torch

# Label mapping from class number to actual category
label_map = {0: "World", 1: "Sports", 2: "Business", 3: "Sci/Tech"}

# Prediction function
def classify_news(title):
    # Tokenize user input
    inputs = tokenizer(
        title,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

    # Predict
    with torch.no_grad():
        outputs = model(**inputs)
        prediction = torch.argmax(outputs.logits, dim=1).item()

    return label_map[prediction]

# Gradio interface
interface = gr.Interface(
    fn=classify_news,
    inputs=gr.Textbox(lines=2, placeholder="Enter a news headline here..."),
    outputs="text",
    title="ðŸ“° News Topic Classifier",
    description="Enter any news headline and this app will predict whether it's about World, Sports, Business, or Sci/Tech."
)

# Launch app
interface.launch(debug=False)
